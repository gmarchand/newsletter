{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5660fe27-27f4-4421-bad1-b7108479b4fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Newsletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e094190e-d137-4ad6-a4a6-affdf2636a79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.203.0 requires uvicorn==0.22.0, but you have uvicorn 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet --requirement requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84edc707-c5a5-484d-8d88-1e838b86ab92",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "import boto3\n",
    "import html2text\n",
    "import streamlit as st\n",
    "from streamlit_gsheets import GSheetsConnection\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b102f-8118-4311-a9f1-e699421c1f02",
   "metadata": {},
   "source": [
    "## Import Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760db1c9-6e92-406f-b8a7-b7fbf2ee9dd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 16:11:27.510 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-01-25 16:11:27.528 No runtime found, using MemoryCacheStorageManager\n",
      "2024-01-25 16:11:27.530 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "url = \"https://docs.google.com/spreadsheets/d/1-d9zHiwDtVUvP9v7ZW3O2Ys6f24Yz_W8zKaNb5uy8dY/edit#gid=0\"\n",
    "# Create a connection object.\n",
    "conn = st.connection(\"gsheets\", type=GSheetsConnection)\n",
    "df = conn.read(spreadsheet=url, worksheet=\"0\", ttl=\"24h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6bacb7-522e-4f01-a38f-17dd9c84b9f6",
   "metadata": {},
   "source": [
    "### Clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d545211-1f34-476e-b2cf-67597796cba3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def html_to_text(text):\n",
    "    text = \"\" if isinstance(text,float) else text\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = True\n",
    "    return h.handle(text)\n",
    "\n",
    "df['ArticleContentTxt'] = df['ArticleContent'].apply(html_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb256804-f60e-411e-94fb-81468e02e421",
   "metadata": {},
   "source": [
    "## French title and summary of each blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c25788-5d15-4207-bcac-4bba0a783c28",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelId = \"anthropic.claude-v2:1\"\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-west-2\",\n",
    ")\n",
    "model = Bedrock(\n",
    "    model_id=modelId,\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 4096,\n",
    "        \"stop_sequences\": [],\n",
    "        \"temperature\": 0.5,  # Use a lower value to decrease randomness in the response.\n",
    "        \"top_p\": 1,  # Use a lower value to ignore less probable options.\n",
    "        \"top_k\": 250,  # Specify the number of token choices the model uses to generate the next token.\n",
    "    },\n",
    "    client=bedrock_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f21da5-56d0-4a42-bdf7-3cf940b12ec4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_df_columns_to_list_dicts(dataframe: pd.DataFrame, column_names: List[str]) -> List[dict]:\n",
    "    result = []\n",
    "    for row in dataframe.itertuples():\n",
    "        dictionary = {}\n",
    "        for column_name in column_names:\n",
    "            dictionary[column_name] = getattr(row, column_name)\n",
    "        result.append(dictionary)\n",
    "    return result\n",
    "\n",
    "def transform_list_of_dicts_to_dataframe(dicts):\n",
    "   \n",
    "    # Create an empty DataFrame with appropriate column names\n",
    "    column_names = set(key for dictionary in dicts for key in dictionary.keys())\n",
    "    df_tmp = pd.DataFrame(columns=list(column_names))\n",
    "\n",
    "    # Transform dictionaries to the DataFrame\n",
    "    for dictionary in dicts:\n",
    "        df_tmp = df_tmp._append(dictionary,ignore_index=True)        \n",
    "    return df_tmp\n",
    "\n",
    "\n",
    "new_dicts = transform_df_columns_to_list_dicts(df, ['ArticleTitle', 'ArticleURL'])\n",
    "new_df = transform_list_of_dicts_to_dataframe(new_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5af1c-8612-403a-ac7c-427c5ada948b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### With PyDantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b12b53d-eab8-4aac-9120-8ea77bb7b9ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RssItemTitles(ArticleTitle='Amazon Titan Image Generator Demo - Image Playground | Amazon Web Services', ArticleTitleFrench=\"Démonstration du générateur d'images Titan d'Amazon - terrain de jeu d'images | Amazon Web Services\", ArticleSummaryFrench=\"Le générateur d'images Titan d'Amazon permet aux créateurs de contenu une idéation et une itération rapides résultant en une génération d'images à haute efficacité. Vous pouvez accéder au modèle de base du générateur d'images Titan d'Amazon dans Amazon Bedrock, qui vous aide à construire et à mettre à l'échelle facilement des applications d'IA générative avec de nouvelles capacités de génération et d'édition d'images.\"),\n",
       " RssItemTitles(ArticleTitle='Amazon ECS and AWS Fargate now integrate with Amazon EBS', ArticleTitleFrench=\"Amazon ECS et AWS Fargate s'intègrent désormais avec Amazon EBS\", ArticleSummaryFrench=\"Amazon ECS et AWS Fargate permettent maintenant de provisionner et attacher facilement des volumes EBS aux tâches ECS sur Fargate et EC2 à l'aide des API ECS. Cela facilite le déploiement d'applications de stockage et de données intensives.\"),\n",
       " RssItemTitles(ArticleTitle='Effective data sorting with Amazon DynamoDB', ArticleTitleFrench='Tri efficace des données avec Amazon DynamoDB', ArticleSummaryFrench=\"Cet article explique comment trier efficacement les données stockées dans Amazon DynamoDB à l'aide de tris secondaires. Il décrit également quand et comment utiliser des tris secondaires pour des requêtes plus rapides.\"),\n",
       " RssItemTitles(ArticleTitle='AWS re:Invent 2023 - Creating an interactive AI Twitch streamer on AWS (COM209)', ArticleTitleFrench=\"AWS re:Invent 2023 - Création d'un streamer Twitch IA interactif sur AWS (COM209)\", ArticleSummaryFrench=\"Cet article décrit comment construire un streamer Twitch IA capable d'interactions humaines sur AWS en utilisant Amazon Bedrock, Python et l'API Twitch. Il montre également une démo en direct de la mise en œuvre ainsi qu'une discussion des défis éthiques.\"),\n",
       " RssItemTitles(ArticleTitle='Automating CloudFront Continuous Deployment with a CI/CD Pipeline', ArticleTitleFrench='Automatisation du déploiement continu de CloudFront avec un pipeline CI/CD', ArticleSummaryFrench=\"Cet article explique comment configurer un pipeline CI/CD pour automatiser le déploiement continu de CloudFront afin d'améliorer la vitesse et la fiabilité.\"),\n",
       " RssItemTitles(ArticleTitle='How to leverage Application Load Balancer’s advanced request routing to route application traffic across multiple Amazon EKS clusters', ArticleTitleFrench=\"Comment tirer parti du routage de requêtes avancé d'Application Load Balancer pour router le trafic d'applications sur plusieurs clusters Amazon EKS\", ArticleSummaryFrench=\"Cet article explique comment utiliser les fonctionnalités de routage avancé d'Application Load Balancer pour router le trafic vers des microservices répartis sur plusieurs clusters Amazon EKS dans une région AWS donnée. Il montre également comment combiner cela avec le AWS Load Balancer Controller et la ressource personnalisée TargetGroupBinding pour un routage de trafic efficace.\"),\n",
       " RssItemTitles(ArticleTitle='How Wonder Dynamics is accelerating creativity with AWS', ArticleTitleFrench='Comment Wonder Dynamics accélère la créativité avec AWS', ArticleSummaryFrench=\"Wonder Dynamics utilise l'IA et le cloud AWS pour rendre la création d'effets visuels plus accessible. Leur plateforme Wonder Studio permet aux créateurs de tous niveaux de facilement intégrer des personnages générés par ordinateur dans des scènes en prises de vue réelle.\"),\n",
       " RssItemTitles(ArticleTitle='Host the Whisper Model on Amazon SageMaker: exploring inference options', ArticleTitleFrench=\"Héberger le modèle Whisper sur Amazon SageMaker : explorer les options d'inférence\", ArticleSummaryFrench=\"Cet article explique comment héberger le modèle de reconnaissance vocale Whisper sur Amazon SageMaker en utilisant PyTorch ou Hugging Face, et compare les différentes options d'inférence telles que l'inférence en temps réel, les travaux de transformation par lots et l'inférence asynchrone.\"),\n",
       " RssItemTitles(ArticleTitle='Amazon EKS extended support for Kubernetes versions pricing', ArticleTitleFrench=\"Tarification du support étendu pour les versions de Kubernetes d'Amazon EKS\", ArticleSummaryFrench='Amazon EKS a annoncé la tarification pour le support étendu des versions de Kubernetes, qui sera de 0,60 $ par cluster et par heure. Le support étendu fournit 12 mois de support supplémentaires pour les versions mineures de Kubernetes.'),\n",
       " RssItemTitles(ArticleTitle='OpenSearch Expands Leadership Beyond AWS', ArticleTitleFrench=\"OpenSearch étend son leadership au-delà d'AWS\", ArticleSummaryFrench=\"OpenSearch a lancé son premier comité de direction en décembre 2023, une étape importante vers une gouvernance ouverte. Le projet est prêt à s'étendre au-delà d'AWS pour inclure un leadership représentant un ensemble plus divers de membres.\"),\n",
       " RssItemTitles(ArticleTitle='Unlocking cloud-based quality of experience (QoE) management with TAG and AWS', ArticleTitleFrench=\"Déverrouiller la gestion de la qualité de l'expérience (QoE) basée sur le cloud avec TAG et AWS\", ArticleSummaryFrench=\"Cet article de blog décrit comment la plateforme TAG Realtime Media Performance aide les diffuseurs à maintenir la familiarité opérationnelle avec les workflows de salles de contrôle principales (MCR) traditionnels en tirant parti des services natifs d'AWS pour faciliter les capacités d'alarme de TAG. Il compare également la solution à un workflow de surveillance traditionnel et examine les considérations clés de la conception.\"),\n",
       " RssItemTitles(ArticleTitle='Power neural search with AI/ML connectors in Amazon OpenSearch Service', ArticleTitleFrench='Renforcez la recherche neuronale avec les connecteurs IA / ML dans Amazon OpenSearch Service', ArticleSummaryFrench=\"Cet article explique comment configurer des connecteurs IA/ML vers des modèles externes via la console Amazon OpenSearch Service pour alimenter la recherche sémantique. Il montre également comment créer un index de recherche neuronale en quelques minutes à l'aide d'un pipeline d'ingestion neuronale et de la recherche neuronale utilisant l'ID de modèle pour générer l'embedding de vecteur à la volée pendant l'ingestion et la recherche.\"),\n",
       " RssItemTitles(ArticleTitle='Better understand sports fan data with Fan360 on AWS', ArticleTitleFrench='Mieux comprendre les données des fans de sport avec Fan360 sur AWS', ArticleSummaryFrench='Cet article explique comment les organisations sportives peuvent utiliser une architecture de maillage de données Fan360 pour obtenir de nouvelles informations à partir de données existantes et émergentes. Il se concentre sur la façon dont une architecture de maillage de données fonctionne et comment elle peut permettre une collaboration à grande échelle.'),\n",
       " RssItemTitles(ArticleTitle='Create Fan360 data products on AWS', ArticleTitleFrench='Créer des produits de données Fan360 sur AWS', ArticleSummaryFrench=\"Cet article présente comment une architecture de maillage de données Fan360 peut aider les entités sportives à enrichir les informations grâce à la collaboration des données au sein de l'organisation. En combinant les données de différentes sources, les entités sportives peuvent construire un domaine de données Fan360 avec des produits de données raffinés.\"),\n",
       " RssItemTitles(ArticleTitle='adidas: Building a streaming analytics application with AWS Analytics | AWS Events', ArticleTitleFrench=\"adidas: Construction d'une application d'analyse de streaming avec AWS Analytics | Événements AWS\", ArticleSummaryFrench=\"Paul Vassu, VP de l'ingénierie de plateforme chez adidas, explique comment adidas a construit une application d'analyse de streaming qui aide à changer des vies grâce au sport. L'équipe adidas a travaillé à rebours de son objectif et a développé une application utilisant AWS Kinesis, Amazon QuickSight, Amazon OpenSearch Service et plus encore.\"),\n",
       " RssItemTitles(ArticleTitle='The journey to IPv6 on Amazon EKS: Interoperability scenarios (Part 3)', ArticleTitleFrench=\"Le parcours vers IPv6 sur Amazon EKS : scénarios d'interopérabilité (Partie 3)\", ArticleSummaryFrench=\"Cet article explique comment les clusters Amazon EKS en espace d'adressage IPv6 interagissent et interopèrent avec les réseaux IPv4 ainsi que les services AWS. La couche d'interopérabilité intégrée dual-stack vous permet de commencer à migrer vos charges de travail vers IPv6 de manière progressive à l'aide d'Amazon VPC CNI.\"),\n",
       " RssItemTitles(ArticleTitle='Announcing the Amazon Titan Multimodal Embeddings model in Amazon Bedrock | AWS Events', ArticleTitleFrench='Annonce du modèle Amazon Titan Multimodal Embeddings dans Amazon Bedrock | AWS Events', ArticleSummaryFrench=\"Amazon a annoncé le modèle Amazon Titan Multimodal Embeddings dans Amazon Bedrock pour générer des embeddings multimodaux de texte et d'images. Le modèle peut être personnalisé pour améliorer la compréhension du contenu unique des clients et fournir des résultats de recherche plus pertinents.\"),\n",
       " RssItemTitles(ArticleTitle='RED Camera Cloud Upload to AWS', ArticleTitleFrench='Téléchargement dans le cloud de la caméra RED vers AWS', ArticleSummaryFrench='RED Digital Cinema a introduit la possibilité de télécharger directement vers Amazon S3 depuis ses caméras cinématographiques. Cette intégration permet aux cinéastes de enregistrer des clips et de télécharger immédiatement ce contenu vers AWS sans avoir à retirer les supports de la caméra ou à utiliser un ordinateur externe.'),\n",
       " RssItemTitles(ArticleTitle='Amazon EKS and Amazon EKS Distro now support Kubernetes version 1.29', ArticleTitleFrench='Amazon EKS et Amazon EKS Distro prennent désormais en charge la version 1.29 de Kubernetes', ArticleSummaryFrench='AWS annonce que Amazon EKS et Amazon EKS Distro supportent maintenant Kubernetes version 1.29. Vous pouvez créer de nouveaux clusters EKS utilisant v1.29 et mettre à niveau vos clusters existants vers v1.29.'),\n",
       " RssItemTitles(ArticleTitle='Ateliere and AWS sign SCA for 5 years collaboration', ArticleTitleFrench='Ateliere et AWS signent un accord de collaboration stratégique de 5 ans', ArticleSummaryFrench=\"Ateliere, développeur de solutions natives pour le cloud pour la chaîne d'approvisionnement médiatique, a conclu un accord de collaboration stratégique de 5 ans avec Amazon Web Services. Cette collaboration vise à redéfinir le paysage de la production et de la distribution de médias en introduisant des solutions basées sur le cloud.\"),\n",
       " RssItemTitles(ArticleTitle='Behind the scenes: AWS real-time architecture for Bundesliga Match Facts', ArticleTitleFrench=\"Dans les coulisses : l'architecture en temps réel d'AWS pour les faits de match de la Bundesliga\", ArticleSummaryFrench=\"Cet article explique comment la Bundesliga utilise l'architecture serverless d'AWS pour fournir des statistiques de match en temps réel à 500 millions de fans dans le monde. L'article décrit en détail comment un système basé sur l'architecture orientée événements et la technologie serverless d'AWS permet d'obtenir une solution évolutive et rentable.\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data structure\n",
    "class RssItemTitles(BaseModel):\n",
    "    ArticleTitle: str = Field(description=\"Title of the article\")\n",
    "    ArticleTitleFrench: str = Field(description=\"Title of the article in french\")\n",
    "    ArticleSummaryFrench: str = Field(description=\"Summary of the article in french\")\n",
    "    \n",
    "        \n",
    "\n",
    "translate_template = \"\"\"\n",
    "You will be acting as a Solutions Architect, working for Amazon Web Services. you are a specialist of Cloud technologies and AWS.\n",
    "\n",
    "First, translate in French, the title of this article coming from the AWS Blog.\n",
    "Secondly, create an engaging summary of two sentences in french based of the content of this article coming from the AWS Blog.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "<title>\n",
    "{ArticleTitle}\n",
    "</title>\n",
    "\n",
    "<content>\n",
    "{ArticleContentTxt}\n",
    "</content>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=RssItemTitles)\n",
    "prompt_translate = PromptTemplate.from_template(\n",
    "    template=translate_template,\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "chain = prompt_translate | model | parser\n",
    "#result = chain.invoke({\"ArticleTitle\":\"Automating CloudFront Continuous Deployment with a CI/CD Pipeline\"})\n",
    "results = chain.batch(transform_df_columns_to_list_dicts(df, ['ArticleTitle','ArticleContentTxt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0926a-6c3a-4d09-a2d9-c4a2c96d877b",
   "metadata": {},
   "source": [
    "### With DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c6acfc9-f5ab-475d-a803-3464d220d4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ArticleTitle': 0     Amazon Titan Image Generator Demo - Image Play...\n",
       " 1     Amazon ECS and AWS Fargate now integrate with ...\n",
       " 2           Effective data sorting with Amazon DynamoDB\n",
       " 3     AWS re:Invent 2023 - Creating an interactive A...\n",
       " 4     Automating CloudFront Continuous Deployment wi...\n",
       " 5     How to leverage Application Load Balancer’s ad...\n",
       " 6     How Wonder Dynamics is accelerating creativity...\n",
       " 7     Host the Whisper Model on Amazon SageMaker: ex...\n",
       " 8     Amazon EKS extended support for Kubernetes ver...\n",
       " 9              OpenSearch Expands Leadership Beyond AWS\n",
       " 10    Unlocking cloud-based quality of experience (Q...\n",
       " 11    Power neural search with AI/ML connectors in A...\n",
       " 12    Better understand sports fan data with Fan360 ...\n",
       " 13                   Create Fan360 data products on AWS\n",
       " 14    adidas: Building a streaming analytics applica...\n",
       " 15    The journey to IPv6 on Amazon EKS: Interoperab...\n",
       " 16    Announcing the Amazon Titan Multimodal Embeddi...\n",
       " 17                       RED Camera Cloud Upload to AWS\n",
       " 18    Amazon EKS and Amazon EKS Distro now support K...\n",
       " 19    Ateliere and AWS sign SCA for 5 years collabor...\n",
       " 20    Behind the scenes: AWS real-time architecture ...\n",
       " Name: ArticleTitle, dtype: object}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = PandasDataFrameOutputParser(dataframe=df)\n",
    "\n",
    "df_template = \"\"\"\n",
    "You will be acting as a Solutions Architect, working for Amazon Web Services. you are a specialist of Cloud technologies and AWS.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "{query}\n",
    "\n",
    "\"\"\"\n",
    "# Set up the prompt.\n",
    "prompt = PromptTemplate(\n",
    "    template=df_template,\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": \"translate in French, each line of the column ArticleTitle.\"})\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30749dad-6b19-401f-a188-885483ea44a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
